{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd93c49a",
   "metadata": {},
   "source": [
    "# 本文将用于从Tensorflow/TFLite中提取Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a5c7f8",
   "metadata": {},
   "source": [
    "## For mobilenetv1_128_0.25模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0044234",
   "metadata": {},
   "source": [
    "## 1.从Tensorflow(.h5 File)中提取Conv2D的Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e64ac9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T10:56:20.654004Z",
     "start_time": "2022-08-16T10:56:20.292935Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#以下为一个提取Tensorflow Conv2d卷积weights+bias的函数，会以txt保存在程序目录下\n",
    "def OpenNNA_Conv2d_GetWeights_From_TF(Layer_Number,Para,Weights_Index,Bias_Index,precision):\n",
    "    data_t = precision#\"%8.6f\"\n",
    "    weights_file_path = \"./\"+str(Layer_Number)+\"_conv2d_\"+str(Layer_Number)+\"_weights.txt\"\n",
    "    Weights   = weights[Weights_Index]\n",
    "    bias_file_path = \"./\"+str(Layer_Number)+\"_conv2d_\"+str(Layer_Number)+\"_bias.txt\"\n",
    "    Bias = weights[Bias_Index]\n",
    "\n",
    "    Conv2d_Kernel_size_row = Para[0] #卷积核大小\n",
    "    Conv2d_Kernel_size_col = Para[1] #卷积核大小\n",
    "    Conv2d_Kernel_channel = Para[2] #卷积核通道\n",
    "    Conv2d_Kernel_number = Para[3] #卷积核数量\n",
    "    Conv2d_bias_number = Para[4]  #偏置数量\n",
    "\n",
    "    #tensorflow hwc -> chw\n",
    "    loadData0 = np.swapaxes(Weights, 0, 2)\n",
    "    loadData1 = np.swapaxes(loadData0, 1, 3)\n",
    "    loadData2 = np.swapaxes(loadData1, 0, 1)\n",
    "    txtfile0 = open(weights_file_path, 'w',encoding='UTF-8')\n",
    "    for i in range(0,Conv2d_Kernel_number):\n",
    "        txtfile0.write(\"\\t{\\n\")\n",
    "        for j in range(0,Conv2d_Kernel_channel):\n",
    "            txtfile0.write(\"\\t\\t{\\n\")\n",
    "            for k in range(0,Conv2d_Kernel_size_row):\n",
    "                txtfile0.write(\"\\t\\t\\t{\")\n",
    "                for w in range(0,Conv2d_Kernel_size_col):\n",
    "                   txtfile0.write(data_t%(loadData2[i][j][k][w])+\", \") \n",
    "                txtfile0.write(\"},\\n\")  \n",
    "            txtfile0.write(\"\\t\\t},\\n\")\n",
    "        txtfile0.write(\"\\t},\\n\\n\")\n",
    "    txtfile0.close()\n",
    "    #Bias\n",
    "    txtfile1 = open(bias_file_path, 'w',encoding='UTF-8')\n",
    "    txtfile1.write(\"\\t{\")\n",
    "    for i in range(0,Conv2d_bias_number):\n",
    "        txtfile1.write(data_t%(Bias[i])+\", \")\n",
    "    txtfile1.write(\"}\")\n",
    "    txtfile1.close()\n",
    "    print(\"OpenNNA: Conv2d Layer:%d Weights[%d]+Bias[%d] Get Success!\"%(Layer_Number,Weights_Index,Bias_Index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a88ce9",
   "metadata": {},
   "source": [
    "## 2.从Tensorflow(.h5 File)中提取Dense的Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5a64846",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T10:58:09.470969Z",
     "start_time": "2022-08-16T10:58:09.461003Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#以下为一个提取Tensorflow Dense卷积weights+bias的函数，会以txt保存在程序目录下\n",
    "def OpenNNA_Dense_GetWeights_From_TF(Layer_Number,Para,Weights_Index,Bias_Index,precision):\n",
    "    data_t = precision#\"%8.6f\"\n",
    "    weights_file_path = \"./\"+str(Layer_Number)+\"_dense_\"+str(Layer_Number)+\"_weights.txt\"\n",
    "    Weights   = weights[Weights_Index]\n",
    "    bias_file_path = \"./\"+str(Layer_Number)+\"_dense_\"+str(Layer_Number)+\"_bias.txt\"\n",
    "    Bias = weights[Bias_Index]\n",
    "    #tensorflow默认的weights存储格式为HWC，在这里需要转为OpenNNA支持的CHW\n",
    "    Dense_Input_Channel = Para[0] #神经元输入channel c\n",
    "    Dense_Input_Row = Para[1] #神经元输入row h\n",
    "    Dense_Input_Col = Para[2] #神经元输入col w\n",
    "    Dense_Units = Para[3] #神经元数量\n",
    "    Dense_bias_number = Para[4]  #偏置数量\n",
    "\n",
    "    dense1_weights = np.swapaxes(Weights,0,1)\n",
    "    txtfile_1 = open(weights_file_path, 'w', encoding='UTF-8')\n",
    "    txtfile_1.write(\"\\n{\")\n",
    "    for i in range(0,Dense_Units):\n",
    "        txtfile_1.write(\"\\n\\t{\")\n",
    "        for j in range(0,Dense_Input_Channel):#c\n",
    "            for k in range(0,Dense_Input_Row):#h\n",
    "                for l in range(0,Dense_Input_Col):#w\n",
    "                    txtfile_1.write(data_t%dense1_weights[i][j+l*Dense_Input_Channel+k*Dense_Input_Channel*Dense_Input_Col]+\",\")\n",
    "        txtfile_1.write(\"},\\n\")\n",
    "    txtfile_1.write(\"\\n}\")\n",
    "    txtfile_1.close()\n",
    "\n",
    "    #提取第一层Dense的bias\n",
    "    txtfile_1 = open(bias_file_path, 'w', encoding='UTF-8')\n",
    "    txtfile_1.write(\"\\n{\")\n",
    "    for i in range(0,Dense_bias_number):\n",
    "        txtfile_1.write(data_t%Bias[i]+\",\")\n",
    "    txtfile_1.write(\"}\")\n",
    "    txtfile_1.close()\n",
    "    print(\"OpenNNA: Dense Layer:%d Weights[%d]+Bias[%d] Get Success!\"%(Layer_Number,Weights_Index,Bias_Index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9211bd3",
   "metadata": {},
   "source": [
    "## 3.加载训练好的模型文件(.h5 File)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c587ba0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T11:05:48.584349Z",
     "start_time": "2022-08-16T11:05:42.461345Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('Conv(OpenNNA-Paper)-Mnist-Int8.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435820b2",
   "metadata": {},
   "source": [
    "## 4.加载TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35c18e73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T12:13:46.215286Z",
     "start_time": "2022-08-25T12:13:40.308699Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Input details ==\n",
      "name: input\n",
      "shape: [  1 128 128   3]\n",
      "type: <class 'numpy.uint8'>\n",
      "\n",
      "\n",
      "== Output details ==\n",
      "name: MobilenetV1/Predictions/Reshape_1\n",
      "shape: [   1 1001]\n",
      "type: <class 'numpy.uint8'>\n",
      "\n",
      "0 | <class 'type'> | MobilenetV1/Logits/AvgPool_1a/AvgPool | Scales: [0.02352848] | zero_points [0] | (1, 1, 1, 256)\n",
      "1 | <class 'type'> | MobilenetV1/Logits/Conv2d_1c_1x1/BiasAdd | Scales: [0.13083284] | zero_points [96] | (1, 1, 1, 1001)\n",
      "2 | <class 'type'> | MobilenetV1/Logits/Conv2d_1c_1x1/Conv2D_bias | Scales: [0.00013035] | zero_points [0] | (1001,)\n",
      "3 | <class 'type'> | MobilenetV1/Logits/Conv2d_1c_1x1/weights_quant/FakeQuantWithMinMaxVars | Scales: [0.00554029] | zero_points [94] | (1001, 1, 1, 256)\n",
      "4 | <class 'type'> | MobilenetV1/Logits/SpatialSqueeze | Scales: [0.13083284] | zero_points [96] | (1, 1001)\n",
      "5 | <class 'type'> | MobilenetV1/Logits/SpatialSqueeze_shape | Scales: [] | zero_points [] | (2,)\n",
      "6 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_0/Conv2D_Fold_bias | Scales: [6.939383e-05] | zero_points [0] | (8,)\n",
      "7 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_0/Relu6 | Scales: [0.02352848] | zero_points [0] | (1, 64, 64, 8)\n",
      "8 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_0/weights_quant/FakeQuantWithMinMaxVars | Scales: [0.00888241] | zero_points [157] | (8, 3, 3, 3)\n",
      "9 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_10_depthwise/Relu6 | Scales: [0.02352848] | zero_points [0] | (1, 8, 8, 128)\n",
      "10 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_10_depthwise/depthwise_Fold_bias | Scales: [0.00053594] | zero_points [0] | (128,)\n",
      "11 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_10_depthwise/weights_quant/FakeQuantWithMinMaxVars | Scales: [0.02277817] | zero_points [135] | (1, 3, 3, 128)\n",
      "12 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_10_pointwise/Conv2D_Fold_bias | Scales: [0.00017001] | zero_points [0] | (128,)\n",
      "13 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_10_pointwise/Relu6 | Scales: [0.02352848] | zero_points [0] | (1, 8, 8, 128)\n",
      "14 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_10_pointwise/weights_quant/FakeQuantWithMinMaxVars | Scales: [0.00722568] | zero_points [131] | (128, 1, 1, 128)\n",
      "15 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_11_depthwise/Relu6 | Scales: [0.02352848] | zero_points [0] | (1, 8, 8, 128)\n",
      "16 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_11_depthwise/depthwise_Fold_bias | Scales: [0.00042958] | zero_points [0] | (128,)\n",
      "17 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_11_depthwise/weights_quant/FakeQuantWithMinMaxVars | Scales: [0.01825803] | zero_points [147] | (1, 3, 3, 128)\n",
      "18 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_11_pointwise/Conv2D_Fold_bias | Scales: [0.00018328] | zero_points [0] | (128,)\n",
      "19 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_11_pointwise/Relu6 | Scales: [0.02352848] | zero_points [0] | (1, 8, 8, 128)\n",
      "20 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_11_pointwise/weights_quant/FakeQuantWithMinMaxVars | Scales: [0.00778962] | zero_points [109] | (128, 1, 1, 128)\n",
      "21 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_12_depthwise/Relu6 | Scales: [0.02352848] | zero_points [0] | (1, 4, 4, 128)\n",
      "22 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_12_depthwise/depthwise_Fold_bias | Scales: [0.00013574] | zero_points [0] | (128,)\n",
      "23 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_12_depthwise/weights_quant/FakeQuantWithMinMaxVars | Scales: [0.00576935] | zero_points [125] | (1, 3, 3, 128)\n",
      "24 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_12_pointwise/Conv2D_Fold_bias | Scales: [0.0002344] | zero_points [0] | (256,)\n",
      "25 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_12_pointwise/Relu6 | Scales: [0.02352848] | zero_points [0] | (1, 4, 4, 256)\n",
      "26 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_12_pointwise/weights_quant/FakeQuantWithMinMaxVars | Scales: [0.00996231] | zero_points [113] | (256, 1, 1, 128)\n",
      "27 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_13_depthwise/Relu6 | Scales: [0.02352848] | zero_points [0] | (1, 4, 4, 256)\n",
      "28 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_13_depthwise/depthwise_Fold_bias | Scales: [0.0006068] | zero_points [0] | (256,)\n",
      "29 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_13_depthwise/weights_quant/FakeQuantWithMinMaxVars | Scales: [0.02579006] | zero_points [165] | (1, 3, 3, 256)\n",
      "30 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_13_pointwise/Conv2D_Fold_bias | Scales: [0.00055018] | zero_points [0] | (256,)\n",
      "31 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_13_pointwise/Relu6 | Scales: [0.02352848] | zero_points [0] | (1, 4, 4, 256)\n",
      "32 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_13_pointwise/weights_quant/FakeQuantWithMinMaxVars | Scales: [0.02338353] | zero_points [144] | (256, 1, 1, 256)\n",
      "33 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_1_depthwise/Relu6 | Scales: [0.02352848] | zero_points [0] | (1, 64, 64, 8)\n",
      "34 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_1_depthwise/depthwise_Fold_bias | Scales: [0.00210345] | zero_points [0] | (8,)\n",
      "35 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_1_depthwise/weights_quant/FakeQuantWithMinMaxVars | Scales: [0.08940034] | zero_points [204] | (1, 3, 3, 8)\n",
      "36 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_1_pointwise/Conv2D_Fold_bias | Scales: [0.00037859] | zero_points [0] | (16,)\n",
      "37 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_1_pointwise/Relu6 | Scales: [0.02352848] | zero_points [0] | (1, 64, 64, 16)\n",
      "38 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_1_pointwise/weights_quant/FakeQuantWithMinMaxVars | Scales: [0.01609091] | zero_points [120] | (16, 1, 1, 8)\n",
      "39 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_2_depthwise/Relu6 | Scales: [0.02352848] | zero_points [0] | (1, 32, 32, 16)\n",
      "40 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_2_depthwise/depthwise_Fold_bias | Scales: [0.00112924] | zero_points [0] | (16,)\n",
      "41 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_2_depthwise/weights_quant/FakeQuantWithMinMaxVars | Scales: [0.04799445] | zero_points [204] | (1, 3, 3, 16)\n",
      "42 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_2_pointwise/Conv2D_Fold_bias | Scales: [0.0011218] | zero_points [0] | (32,)\n",
      "43 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_2_pointwise/Relu6 | Scales: [0.02352848] | zero_points [0] | (1, 32, 32, 32)\n",
      "44 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_2_pointwise/weights_quant/FakeQuantWithMinMaxVars | Scales: [0.04767826] | zero_points [120] | (32, 1, 1, 16)\n",
      "45 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_3_depthwise/Relu6 | Scales: [0.02352848] | zero_points [0] | (1, 32, 32, 32)\n",
      "46 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_3_depthwise/depthwise_Fold_bias | Scales: [0.00080069] | zero_points [0] | (32,)\n",
      "47 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_3_depthwise/weights_quant/FakeQuantWithMinMaxVars | Scales: [0.03403055] | zero_points [149] | (1, 3, 3, 32)\n",
      "48 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_3_pointwise/Conv2D_Fold_bias | Scales: [0.00054806] | zero_points [0] | (32,)\n",
      "49 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_3_pointwise/Relu6 | Scales: [0.02352848] | zero_points [0] | (1, 32, 32, 32)\n",
      "50 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_3_pointwise/weights_quant/FakeQuantWithMinMaxVars | Scales: [0.02329343] | zero_points [61] | (32, 1, 1, 32)\n",
      "51 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_4_depthwise/Relu6 | Scales: [0.02352848] | zero_points [0] | (1, 16, 16, 32)\n",
      "52 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_4_depthwise/depthwise_Fold_bias | Scales: [0.00028911] | zero_points [0] | (32,)\n",
      "53 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_4_depthwise/weights_quant/FakeQuantWithMinMaxVars | Scales: [0.01228761] | zero_points [122] | (1, 3, 3, 32)\n",
      "54 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_4_pointwise/Conv2D_Fold_bias | Scales: [0.00028334] | zero_points [0] | (64,)\n",
      "55 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_4_pointwise/Relu6 | Scales: [0.02352848] | zero_points [0] | (1, 16, 16, 64)\n",
      "56 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_4_pointwise/weights_quant/FakeQuantWithMinMaxVars | Scales: [0.01204246] | zero_points [148] | (64, 1, 1, 32)\n",
      "57 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_5_depthwise/Relu6 | Scales: [0.02352848] | zero_points [0] | (1, 16, 16, 64)\n",
      "58 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_5_depthwise/depthwise_Fold_bias | Scales: [0.00072065] | zero_points [0] | (64,)\n",
      "59 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_5_depthwise/weights_quant/FakeQuantWithMinMaxVars | Scales: [0.03062893] | zero_points [102] | (1, 3, 3, 64)\n",
      "60 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_5_pointwise/Conv2D_Fold_bias | Scales: [0.00037503] | zero_points [0] | (64,)\n",
      "61 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_5_pointwise/Relu6 | Scales: [0.02352848] | zero_points [0] | (1, 16, 16, 64)\n",
      "62 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_5_pointwise/weights_quant/FakeQuantWithMinMaxVars | Scales: [0.01593948] | zero_points [89] | (64, 1, 1, 64)\n",
      "63 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_6_depthwise/Relu6 | Scales: [0.02352848] | zero_points [0] | (1, 8, 8, 64)\n",
      "64 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_6_depthwise/depthwise_Fold_bias | Scales: [0.0001939] | zero_points [0] | (64,)\n",
      "65 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_6_depthwise/weights_quant/FakeQuantWithMinMaxVars | Scales: [0.00824124] | zero_points [125] | (1, 3, 3, 64)\n",
      "66 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_6_pointwise/Conv2D_Fold_bias | Scales: [0.00031384] | zero_points [0] | (128,)\n",
      "67 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_6_pointwise/Relu6 | Scales: [0.02352848] | zero_points [0] | (1, 8, 8, 128)\n",
      "68 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_6_pointwise/weights_quant/FakeQuantWithMinMaxVars | Scales: [0.01333861] | zero_points [158] | (128, 1, 1, 64)\n",
      "69 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_7_depthwise/Relu6 | Scales: [0.02352848] | zero_points [0] | (1, 8, 8, 128)\n",
      "70 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_7_depthwise/depthwise_Fold_bias | Scales: [0.00068484] | zero_points [0] | (128,)\n",
      "71 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_7_depthwise/weights_quant/FakeQuantWithMinMaxVars | Scales: [0.0291069] | zero_points [122] | (1, 3, 3, 128)\n",
      "72 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_7_pointwise/Conv2D_Fold_bias | Scales: [0.00016808] | zero_points [0] | (128,)\n",
      "73 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_7_pointwise/Relu6 | Scales: [0.02352848] | zero_points [0] | (1, 8, 8, 128)\n",
      "74 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_7_pointwise/weights_quant/FakeQuantWithMinMaxVars | Scales: [0.00714347] | zero_points [134] | (128, 1, 1, 128)\n",
      "75 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_8_depthwise/Relu6 | Scales: [0.02352848] | zero_points [0] | (1, 8, 8, 128)\n",
      "76 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_8_depthwise/depthwise_Fold_bias | Scales: [0.00072821] | zero_points [0] | (128,)\n",
      "77 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_8_depthwise/weights_quant/FakeQuantWithMinMaxVars | Scales: [0.03095001] | zero_points [165] | (1, 3, 3, 128)\n",
      "78 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_8_pointwise/Conv2D_Fold_bias | Scales: [0.00017903] | zero_points [0] | (128,)\n",
      "79 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_8_pointwise/Relu6 | Scales: [0.02352848] | zero_points [0] | (1, 8, 8, 128)\n",
      "80 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_8_pointwise/weights_quant/FakeQuantWithMinMaxVars | Scales: [0.00760898] | zero_points [134] | (128, 1, 1, 128)\n",
      "81 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_9_depthwise/Relu6 | Scales: [0.02352848] | zero_points [0] | (1, 8, 8, 128)\n",
      "82 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_9_depthwise/depthwise_Fold_bias | Scales: [0.00049948] | zero_points [0] | (128,)\n",
      "83 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_9_depthwise/weights_quant/FakeQuantWithMinMaxVars | Scales: [0.02122881] | zero_points [120] | (1, 3, 3, 128)\n",
      "84 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_9_pointwise/Conv2D_Fold_bias | Scales: [0.00026096] | zero_points [0] | (128,)\n",
      "85 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_9_pointwise/Relu6 | Scales: [0.02352848] | zero_points [0] | (1, 8, 8, 128)\n",
      "86 | <class 'type'> | MobilenetV1/MobilenetV1/Conv2d_9_pointwise/weights_quant/FakeQuantWithMinMaxVars | Scales: [0.01109144] | zero_points [146] | (128, 1, 1, 128)\n",
      "87 | <class 'type'> | MobilenetV1/Predictions/Reshape_1 | Scales: [0.00390625] | zero_points [0] | (1, 1001)\n",
      "88 | <class 'type'> | input | Scales: [0.0078125] | zero_points [128] | (1, 128, 128, 3)\n",
      "89 | <class 'type'> |  | Scales: [] | zero_points [] | (1, 64, 64, 27)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tflite_model_name = 'mobilenet_v1_0.25_128_quant.tflite'\n",
    "'''\n",
    "Create interpreter, allocate tensors\n",
    "'''\n",
    "tflite_interpreter = tf.lite.Interpreter(model_path=tflite_model_name)\n",
    "tflite_interpreter.allocate_tensors()\n",
    "\n",
    "'''\n",
    "Check input/output details\n",
    "'''\n",
    "input_details = tflite_interpreter.get_input_details()\n",
    "output_details = tflite_interpreter.get_output_details()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(\"name:\", input_details[0]['name'])\n",
    "print(\"shape:\", input_details[0]['shape'])\n",
    "print(\"type:\", input_details[0]['dtype'])\n",
    "print(\"\\n\\n== Output details ==\")\n",
    "print(\"name:\", output_details[0]['name'])\n",
    "print(\"shape:\", output_details[0]['shape'])\n",
    "print(\"type:\", output_details[0]['dtype'])\n",
    "print()\n",
    "'''\n",
    "Run prediction (optional), input_array has input's shape and dtype\n",
    "'''\n",
    "'''\n",
    "tflite_interpreter.set_tensor(input_details[0]['index'], input_array)\n",
    "tflite_interpreter.invoke()\n",
    "output_array = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "'''\n",
    "'''\n",
    "This gives a list of dictionaries. \n",
    "'''\n",
    "tensor_details = tflite_interpreter.get_tensor_details()\n",
    "\n",
    "for dict in tensor_details:\n",
    "    i = dict['index']\n",
    "    tensor_name = dict['name']\n",
    "    scales = dict['quantization_parameters']['scales']\n",
    "    zero_points = dict['quantization_parameters']['zero_points']\n",
    "    tensor = tflite_interpreter.tensor(i)()\n",
    "\n",
    "    print(i, \"|\",type, \"|\",tensor_name, \"|\",\"Scales:\",scales, \"|\",\"zero_points\",zero_points, \"|\",tensor.shape)\n",
    "    #print(tensor)\n",
    "    '''\n",
    "    See note below\n",
    "    '''\n",
    "#print(\"\\n\\n\\n\")\n",
    "#print(tflite_interpreter.tensor(2)().shape)#tensor_details排布:List套字典"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd487242",
   "metadata": {},
   "source": [
    "## 5.从Tensorflow(.tflite File)中提取Dense的Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e391338c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-22T11:34:46.950647Z",
     "start_time": "2022-08-22T11:34:46.940647Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#以下为一个提取Tensorflow Dense卷积weights+bias的函数，会以txt保存在程序目录下\n",
    "def OpenNNA_Dense_GetWeights_From_TFLite(Layer_Number,Para,Weights_Index,Bias_Index,precision):\n",
    "    tflite_interpreter = tf.lite.Interpreter(model_path=tflite_model_name)\n",
    "    tflite_interpreter.allocate_tensors()\n",
    "    data_t = precision#\"%8.6f\"\n",
    "    weights_file_path = \"./\"+str(Layer_Number)+\"_dense_\"+str(Layer_Number)+\"_weights.txt\"\n",
    "    Weights   = tflite_interpreter.tensor(Weights_Index)()\n",
    "    bias_file_path = \"./\"+str(Layer_Number)+\"_dense_\"+str(Layer_Number)+\"_bias.txt\"\n",
    "    Bias = tflite_interpreter.tensor(Bias_Index)()\n",
    "    #tflite默认的weights存储格式为HWC，在这里需要转为OpenNNA支持的CHW\n",
    "    Dense_Input_Channel = Para[0] #神经元输入channel c\n",
    "    Dense_Input_Row = Para[1] #神经元输入row h\n",
    "    Dense_Input_Col = Para[2] #神经元输入col w\n",
    "    Dense_Units = Para[3] #神经元数量\n",
    "    Dense_bias_number = Para[4]  #偏置数量\n",
    "    #print(Weights.shape)\n",
    "    dense1_weights = Weights#np.swapaxes(Weights,0,1)\n",
    "    #dense1_weights = np.swapaxes(Weights,0,1)\n",
    "    print(dense1_weights.shape)\n",
    "    txtfile_1 = open(weights_file_path, 'w', encoding='UTF-8')\n",
    "    txtfile_1.write(\"\\n{\")\n",
    "    for i in range(0,Dense_Units):\n",
    "        txtfile_1.write(\"\\n\\t{\")\n",
    "        for j in range(0,Dense_Input_Channel):#c\n",
    "            for k in range(0,Dense_Input_Row):#h\n",
    "                for l in range(0,Dense_Input_Col):#w\n",
    "                    # HWC -> CHW\n",
    "                    txtfile_1.write(data_t%dense1_weights[i][j+l*Dense_Input_Channel+k*Dense_Input_Channel*Dense_Input_Col]+\",\")\n",
    "                    #CHW -> CHW\n",
    "                    #txtfile_1.write(data_t%dense1_weights[i][l+k*Dense_Input_Col+j*Dense_Input_Col*Dense_Input_Row]+\",\")\n",
    "        txtfile_1.write(\"},\\n\")\n",
    "    txtfile_1.write(\"\\n}\")\n",
    "    txtfile_1.close()\n",
    "\n",
    "    #提取第一层Dense的bias\n",
    "    txtfile_1 = open(bias_file_path, 'w', encoding='UTF-8')\n",
    "    txtfile_1.write(\"\\n{\")\n",
    "    for i in range(0,Dense_bias_number):\n",
    "        txtfile_1.write(data_t%Bias[i]+\",\")\n",
    "    txtfile_1.write(\"}\")\n",
    "    txtfile_1.close()\n",
    "    print(\"OpenNNA: Dense Layer:%d Weights[%d]+Bias[%d] Get Success!\"%(Layer_Number,Weights_Index,Bias_Index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7892d3fd",
   "metadata": {},
   "source": [
    "## 6.从Tensorflow(.tflite File)中提取Conv2D的Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45e7d102",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T05:14:38.887887Z",
     "start_time": "2022-08-24T05:14:38.870887Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#以下为一个提取Tensorflow Conv2d卷积weights+bias的函数，会以txt保存在程序目录下\n",
    "def OpenNNA_Conv2d_GetWeights_From_TFLite(Layer_Number,Para,Weights_Index,Bias_Index,precision):\n",
    "    tflite_interpreter = tf.lite.Interpreter(model_path=tflite_model_name)\n",
    "    tflite_interpreter.allocate_tensors()\n",
    "    data_t = precision#\"%8.6f\"\n",
    "    weights_file_path = \"./\"+str(Layer_Number)+\"_conv2d_\"+str(Layer_Number)+\"_weights.txt\"\n",
    "    Weights   = tflite_interpreter.tensor(Weights_Index)()\n",
    "    bias_file_path = \"./\"+str(Layer_Number)+\"_conv2d_\"+str(Layer_Number)+\"_bias.txt\"\n",
    "    Bias = tflite_interpreter.tensor(Bias_Index)()\n",
    "\n",
    "    Conv2d_Kernel_size_row = Para[0] #卷积核大小\n",
    "    Conv2d_Kernel_size_col = Para[1] #卷积核大小\n",
    "    Conv2d_Kernel_channel = Para[2] #卷积核通道\n",
    "    Conv2d_Kernel_number = Para[3] #卷积核数量\n",
    "    Conv2d_bias_number = Para[4]  #偏置数量\n",
    "\n",
    "    #tensorflow hwc -> chw\n",
    "    loadData0 = np.swapaxes(Weights, 2, 3)\n",
    "    loadData2 = np.swapaxes(loadData0, 1, 2)\n",
    "    #loadData2 = np.swapaxes(loadData1, 0, 1)\n",
    "    txtfile0 = open(weights_file_path, 'w',encoding='UTF-8')\n",
    "    for i in range(0,Conv2d_Kernel_number):\n",
    "        txtfile0.write(\"\\t{\\n\")\n",
    "        for j in range(0,Conv2d_Kernel_channel):\n",
    "            txtfile0.write(\"\\t\\t{\\n\")\n",
    "            for k in range(0,Conv2d_Kernel_size_row):\n",
    "                txtfile0.write(\"\\t\\t\\t{\")\n",
    "                for w in range(0,Conv2d_Kernel_size_col):\n",
    "                   txtfile0.write(data_t%(loadData2[i][j][k][w])+\", \") \n",
    "                txtfile0.write(\"},\\n\")  \n",
    "            txtfile0.write(\"\\t\\t},\\n\")\n",
    "        txtfile0.write(\"\\t},\\n\\n\")\n",
    "    txtfile0.close()\n",
    "    #Bias\n",
    "    txtfile1 = open(bias_file_path, 'w',encoding='UTF-8')\n",
    "    txtfile1.write(\"\\t{\")\n",
    "    for i in range(0,Conv2d_bias_number):\n",
    "        txtfile1.write(data_t%(Bias[i])+\", \")\n",
    "    txtfile1.write(\"}\")\n",
    "    txtfile1.close()\n",
    "    print(\"OpenNNA: Conv2d Layer:%d Weights[%d]+Bias[%d] Get Success!\"%(Layer_Number,Weights_Index,Bias_Index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d43e1d",
   "metadata": {},
   "source": [
    "## 7.从Tensorflow(.tflite File)中提取Depthwise Conv2D的Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdffb93d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T05:14:41.460788Z",
     "start_time": "2022-08-24T05:14:41.446789Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#以下为一个提取Tensorflow Conv2d卷积weights+bias的函数，会以txt保存在程序目录下\n",
    "def OpenNNA_DWConv2d_GetWeights_From_TFLite(Layer_Number,Para,Weights_Index,Bias_Index,precision):\n",
    "    tflite_interpreter = tf.lite.Interpreter(model_path=tflite_model_name)\n",
    "    tflite_interpreter.allocate_tensors()\n",
    "    data_t = precision#\"%8.6f\"\n",
    "    weights_file_path = \"./\"+str(Layer_Number)+\"_dwconv2d_\"+str(Layer_Number)+\"_weights.txt\"\n",
    "    Weights   = tflite_interpreter.tensor(Weights_Index)()\n",
    "    bias_file_path = \"./\"+str(Layer_Number)+\"_dwconv2d_\"+str(Layer_Number)+\"_bias.txt\"\n",
    "    Bias = tflite_interpreter.tensor(Bias_Index)()\n",
    "\n",
    "    Conv2d_Kernel_size_row = Para[0] #卷积核大小\n",
    "    Conv2d_Kernel_size_col = Para[1] #卷积核大小\n",
    "    Conv2d_Kernel_channel = Para[2] #卷积核通道\n",
    "    Conv2d_Kernel_number = Para[3] #卷积核数量\n",
    "    Conv2d_bias_number = Para[4]  #偏置数量\n",
    "\n",
    "    #tensorflow hwc -> chw\n",
    "    loadData0 = np.swapaxes(Weights, 2, 3)\n",
    "    loadData2 = np.swapaxes(loadData0, 1, 2)\n",
    "    #loadData2 = np.swapaxes(loadData1, 0, 1)\n",
    "    txtfile0 = open(weights_file_path, 'w',encoding='UTF-8')\n",
    "    for i in range(0,Conv2d_Kernel_number):\n",
    "        txtfile0.write(\"\\t{\\n\")\n",
    "        for j in range(0,Conv2d_Kernel_channel):\n",
    "            txtfile0.write(\"\\t\\t{\\n\")\n",
    "            for k in range(0,Conv2d_Kernel_size_row):\n",
    "                txtfile0.write(\"\\t\\t\\t{\")\n",
    "                for w in range(0,Conv2d_Kernel_size_col):\n",
    "                   txtfile0.write(data_t%(loadData2[i][j][k][w])+\", \") \n",
    "                txtfile0.write(\"},\\n\")  \n",
    "            txtfile0.write(\"\\t\\t},\\n\")\n",
    "        txtfile0.write(\"\\t},\\n\\n\")\n",
    "    txtfile0.close()\n",
    "    #Bias\n",
    "    txtfile1 = open(bias_file_path, 'w',encoding='UTF-8')\n",
    "    txtfile1.write(\"\\t{\")\n",
    "    for i in range(0,Conv2d_bias_number):\n",
    "        txtfile1.write(data_t%(Bias[i])+\", \")\n",
    "    txtfile1.write(\"}\")\n",
    "    txtfile1.close()\n",
    "    print(\"OpenNNA: Depthwise Conv2d Layer:%d Weights[%d]+Bias[%d] Get Success!\"%(Layer_Number,Weights_Index,Bias_Index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f59e4883",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T05:14:43.424831Z",
     "start_time": "2022-08-24T05:14:42.764857Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenNNA: Conv2d Layer:0 Weights[8]+Bias[6] Get Success!\n",
      "OpenNNA: Depthwise Conv2d Layer:1 Weights[35]+Bias[34] Get Success!\n",
      "OpenNNA: Conv2d Layer:2 Weights[38]+Bias[36] Get Success!\n",
      "OpenNNA: Depthwise Conv2d Layer:3 Weights[41]+Bias[40] Get Success!\n",
      "OpenNNA: Conv2d Layer:4 Weights[44]+Bias[42] Get Success!\n",
      "OpenNNA: Depthwise Conv2d Layer:5 Weights[47]+Bias[46] Get Success!\n",
      "OpenNNA: Conv2d Layer:6 Weights[50]+Bias[48] Get Success!\n",
      "OpenNNA: Depthwise Conv2d Layer:7 Weights[53]+Bias[52] Get Success!\n",
      "OpenNNA: Conv2d Layer:8 Weights[56]+Bias[54] Get Success!\n",
      "OpenNNA: Depthwise Conv2d Layer:9 Weights[59]+Bias[58] Get Success!\n",
      "OpenNNA: Conv2d Layer:10 Weights[62]+Bias[60] Get Success!\n",
      "OpenNNA: Depthwise Conv2d Layer:11 Weights[65]+Bias[64] Get Success!\n",
      "OpenNNA: Conv2d Layer:12 Weights[68]+Bias[66] Get Success!\n",
      "OpenNNA: Depthwise Conv2d Layer:13 Weights[71]+Bias[70] Get Success!\n",
      "OpenNNA: Conv2d Layer:14 Weights[74]+Bias[72] Get Success!\n",
      "OpenNNA: Depthwise Conv2d Layer:15 Weights[77]+Bias[76] Get Success!\n",
      "OpenNNA: Conv2d Layer:16 Weights[80]+Bias[78] Get Success!\n",
      "OpenNNA: Depthwise Conv2d Layer:17 Weights[83]+Bias[82] Get Success!\n",
      "OpenNNA: Conv2d Layer:18 Weights[86]+Bias[84] Get Success!\n",
      "OpenNNA: Depthwise Conv2d Layer:19 Weights[11]+Bias[10] Get Success!\n",
      "OpenNNA: Conv2d Layer:20 Weights[14]+Bias[12] Get Success!\n",
      "OpenNNA: Depthwise Conv2d Layer:21 Weights[17]+Bias[16] Get Success!\n",
      "OpenNNA: Conv2d Layer:22 Weights[20]+Bias[18] Get Success!\n",
      "OpenNNA: Depthwise Conv2d Layer:23 Weights[23]+Bias[22] Get Success!\n",
      "OpenNNA: Conv2d Layer:24 Weights[26]+Bias[24] Get Success!\n",
      "OpenNNA: Depthwise Conv2d Layer:25 Weights[29]+Bias[28] Get Success!\n",
      "OpenNNA: Conv2d Layer:26 Weights[32]+Bias[30] Get Success!\n",
      "OpenNNA: Conv2d Layer:28 Weights[3]+Bias[2] Get Success!\n"
     ]
    }
   ],
   "source": [
    "#与Tensorflow H5 weights&bias的index需要手动printf检查不同， tflite每层weights&bias的index就是Netron所显示的location\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite(0,[3,3,3,8,8],8,6,\"%d\")#第0层Conv2d\n",
    "\n",
    "OpenNNA_DWConv2d_GetWeights_From_TFLite(1,[3,3,8,1,8],35,34,\"%d\")#第1层Depthwise Conv2d\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite(2,[1,1,8,16,16],38,36,\"%d\")#第2层Conv2d\n",
    "\n",
    "OpenNNA_DWConv2d_GetWeights_From_TFLite(3,[3,3,16,1,16],41,40,\"%d\")#第3层Depthwise Conv2d\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite(4,[1,1,16,32,32],44,42,\"%d\")#第4层Conv2d\n",
    "\n",
    "OpenNNA_DWConv2d_GetWeights_From_TFLite(5,[3,3,32,1,32],47,46,\"%d\")#第5层Depthwise Conv2d\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite(6,[1,1,32,32,32],50,48,\"%d\")#第6层Conv2d\n",
    "\n",
    "OpenNNA_DWConv2d_GetWeights_From_TFLite(7,[3,3,32,1,32],53,52,\"%d\")#第7层Depthwise Conv2d\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite(8,[1,1,32,64,64],56,54,\"%d\")#第8层Conv2d\n",
    "\n",
    "OpenNNA_DWConv2d_GetWeights_From_TFLite(9,[3,3,64,1,64],59,58,\"%d\")#第9层Depthwise Conv2d\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite(10,[1,1,64,64,64],62,60,\"%d\")#第10层Conv2d\n",
    "\n",
    "OpenNNA_DWConv2d_GetWeights_From_TFLite(11,[3,3,64,1,64],65,64,\"%d\")#第11层Depthwise Conv2d\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite(12,[1,1,64,128,128],68,66,\"%d\")#第12层Conv2d\n",
    "\n",
    "OpenNNA_DWConv2d_GetWeights_From_TFLite(13,[3,3,128,1,128],71,70,\"%d\")#第13层Depthwise Conv2d\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite(14,[1,1,128,128,128],74,72,\"%d\")#第14层Conv2d\n",
    "\n",
    "OpenNNA_DWConv2d_GetWeights_From_TFLite(15,[3,3,128,1,128],77,76,\"%d\")#第15层Depthwise Conv2d\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite(16,[1,1,128,128,128],80,78,\"%d\")#第16层Conv2d\n",
    "\n",
    "OpenNNA_DWConv2d_GetWeights_From_TFLite(17,[3,3,128,1,128],83,82,\"%d\")#第17层Depthwise Conv2d\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite(18,[1,1,128,128,128],86,84,\"%d\")#第18层Conv2d\n",
    "\n",
    "OpenNNA_DWConv2d_GetWeights_From_TFLite(19,[3,3,128,1,128],11,10,\"%d\")#第19层Depthwise Conv2d\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite(20,[1,1,128,128,128],14,12,\"%d\")#第20层Conv2d\n",
    "\n",
    "OpenNNA_DWConv2d_GetWeights_From_TFLite(21,[3,3,128,1,128],17,16,\"%d\")#第21层Depthwise Conv2d\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite(22,[1,1,128,128,128],20,18,\"%d\")#第22层Conv2d\n",
    "\n",
    "OpenNNA_DWConv2d_GetWeights_From_TFLite(23,[3,3,128,1,128],23,22,\"%d\")#第23层Depthwise Conv2d\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite(24,[1,1,128,256,256],26,24,\"%d\")#第24层Conv2d\n",
    "\n",
    "OpenNNA_DWConv2d_GetWeights_From_TFLite(25,[3,3,256,1,256],29,28,\"%d\")#第25层Depthwise Conv2d\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite(26,[1,1,256,256,256],32,30,\"%d\")#第26层Conv2d\n",
    "\n",
    "#第27层AvgPool\n",
    "\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite(28,[1,1,256,1001,1001],3,2,\"%d\")#第28层Conv2d\n",
    "\n",
    "#Output:1001\n",
    "\n",
    "\n",
    "#OpenNNA_Dense_GetWeights_From_TFLite(10,[32,6,6,64,64],11,12,\"%d\")#第10层Dense\n",
    "#OpenNNA_Dense_GetWeights_From_TFLite(11,[1,1,64,10,10],13,14,\"%d\")#第11层Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed02de8",
   "metadata": {},
   "source": [
    "## 8.从Tensorflow(.tflite File)中提取Dense的Weights,并生成.h文件\n",
    "与保存到txt文件不同，保存到.h文件将会同时保存当前层的scale和zero point信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b65533e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T14:14:02.661914Z",
     "start_time": "2022-08-24T14:14:02.645217Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#以下为一个提取Tensorflow Dense卷积weights+bias的函数，会以txt保存在程序目录下\n",
    "def OpenNNA_Dense_GetWeights_From_TFLite_Save_HFile(Layer_Number,Para,Weights_Index,Bias_Index,precision):\n",
    "    tflite_interpreter = tf.lite.Interpreter(model_path=tflite_model_name)\n",
    "    tflite_interpreter.allocate_tensors()\n",
    "    data_t = precision#\"%8.6f\"\n",
    "    weights_file_path = \"./L\"+str(Layer_Number)+\"_Dense_Weights\"+\".h\"\n",
    "    Weights   = tflite_interpreter.tensor(Weights_Index)()\n",
    "    bias_file_path = \"./L\"+str(Layer_Number)+\"_Dense_Bias\"+\".h\"\n",
    "    Bias = tflite_interpreter.tensor(Bias_Index)()\n",
    "    #tflite默认的weights存储格式为HWC，在这里需要转为OpenNNA支持的CHW\n",
    "    Dense_Input_Channel = Para[0] #神经元输入channel c\n",
    "    Dense_Input_Row = Para[1] #神经元输入row h\n",
    "    Dense_Input_Col = Para[2] #神经元输入col w\n",
    "    Dense_Units = Para[3] #神经元数量\n",
    "    Dense_bias_number = Para[4]  #偏置数量\n",
    "    #print(Weights.shape)\n",
    "    dense1_weights = Weights#np.swapaxes(Weights,0,1)\n",
    "    #dense1_weights = np.swapaxes(Weights,0,1)\n",
    "    print(dense1_weights.shape)\n",
    "    txtfile_1 = open(weights_file_path, 'w', encoding='UTF-8')\n",
    "    #.h File Header\n",
    "    txtfile_1.write(\"#ifndef __L%d_DENSE_WEIGHTS_H__\\n\"%(Layer_Number))\n",
    "    txtfile_1.write(\"#define __L%d_DENSE_WEIGHTS_H__\\n\"%(Layer_Number))\n",
    "    #数组\n",
    "    txtfile_1.write(\"const Weights_t L%d_Dense_Weights[1][%d][%d]={\\n\"%(Layer_Number,Dense_Units,(Dense_Input_Channel*Dense_Input_Row*Dense_Input_Col)))\n",
    "    #Weights\n",
    "    txtfile_1.write(\"\\n\\t{\")\n",
    "    for i in range(0,Dense_Units):\n",
    "        txtfile_1.write(\"\\n\\t\\t{\")\n",
    "        for j in range(0,Dense_Input_Channel):#c\n",
    "            for k in range(0,Dense_Input_Row):#h\n",
    "                for l in range(0,Dense_Input_Col):#w\n",
    "                    # HWC -> CHW\n",
    "                    txtfile_1.write(data_t%dense1_weights[i][j+l*Dense_Input_Channel+k*Dense_Input_Channel*Dense_Input_Col]+\",\")\n",
    "                    #CHW -> CHW\n",
    "                    #txtfile_1.write(data_t%dense1_weights[i][l+k*Dense_Input_Col+j*Dense_Input_Col*Dense_Input_Row]+\",\")\n",
    "        txtfile_1.write(\"},\\n\")\n",
    "    txtfile_1.write(\"\\n\\t}\")\n",
    "    #.h File End\n",
    "    txtfile_1.write(\"\\n};\")\n",
    "    txtfile_1.write(\"\\n#endif\")\n",
    "    txtfile_1.close()\n",
    "\n",
    "    #提取第一层Dense的bias\n",
    "    txtfile_1 = open(bias_file_path, 'w', encoding='UTF-8')\n",
    "    #.h File Header\n",
    "    txtfile_1.write(\"#ifndef __L%d_DENSE_BIAS_H__\\n\"%(Layer_Number))\n",
    "    txtfile_1.write(\"#define __L%d_DENSE_BIAS_H__\\n\"%(Layer_Number))\n",
    "    #数组\n",
    "    txtfile_1.write(\"const Bias_t L%d_Dense_Bias[%d]={\\n\"%(Layer_Number,Dense_Units))\n",
    "    #Bias\n",
    "    txtfile_1.write(\"\\n\\t\")\n",
    "    for i in range(0,Dense_bias_number):\n",
    "        txtfile_1.write(data_t%Bias[i]+\",\")\n",
    "    txtfile_1.write(\"\\n};\")\n",
    "    txtfile_1.write(\"\\n#endif\")\n",
    "    txtfile_1.close()\n",
    "    print(\"OpenNNA: Dense Layer:%d Weights[%d]+Bias[%d] Get Success!\"%(Layer_Number,Weights_Index,Bias_Index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982b6a16",
   "metadata": {},
   "source": [
    "## 9.从Tensorflow(.tflite File)中提取Depthwise Conv2D的Weights,并生成.h文件\n",
    "与保存到txt文件不同，保存到.h文件将会同时保存当前层的scale和zero point信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0ec7e8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T12:17:25.232653Z",
     "start_time": "2022-08-25T12:17:25.220655Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#以下为一个提取Tensorflow Conv2d卷积weights+bias的函数，会以txt保存在程序目录下\n",
    "def OpenNNA_DWConv2d_GetWeights_From_TFLite_Save_HFile(Layer_Number,Para,Weights_Index,Bias_Index,precision):\n",
    "    tflite_interpreter = tf.lite.Interpreter(model_path=tflite_model_name)\n",
    "    tflite_interpreter.allocate_tensors()\n",
    "    data_t = precision#\"%8.6f\"\n",
    "    weights_file_path = \"./L\"+str(Layer_Number)+\"_DwConv2d_Weights\"+\".h\"\n",
    "    Weights   = tflite_interpreter.tensor(Weights_Index)()\n",
    "    bias_file_path = \"./L\"+str(Layer_Number)+\"_DwConv2d_Bias\"+\".h\"\n",
    "    Bias = tflite_interpreter.tensor(Bias_Index)()\n",
    "\n",
    "    Conv2d_Kernel_size_row = Para[0] #卷积核大小\n",
    "    Conv2d_Kernel_size_col = Para[1] #卷积核大小\n",
    "    Conv2d_Kernel_channel = Para[2] #卷积核通道\n",
    "    Conv2d_Kernel_number = Para[3] #卷积核数量\n",
    "    Conv2d_bias_number = Para[4]  #偏置数量\n",
    "\n",
    "    #tensorflow hwc -> chw\n",
    "    loadData0 = np.swapaxes(Weights, 2, 3)\n",
    "    loadData2 = np.swapaxes(loadData0, 1, 2)\n",
    "    #loadData2 = np.swapaxes(loadData1, 0, 1)\n",
    "    txtfile0 = open(weights_file_path, 'w',encoding='UTF-8')\n",
    "    #.h File Header\n",
    "    txtfile0.write(\"#ifndef __L%d_DWCONV2D_WEIGHTS_H__\\n\"%(Layer_Number))\n",
    "    txtfile0.write(\"#define __L%d_DWCONV2D_WEIGHTS_H__\\n\"%(Layer_Number))\n",
    "    txtfile0.write(\"#include \\\"opennna_core.h\\\"\\n\")\n",
    "    #数组\n",
    "    txtfile0.write(\"const Weights_t L%d_DwConv2d_Weights[%d][%d][%d][%d]={\\n\"%(Layer_Number,Conv2d_Kernel_number,Conv2d_Kernel_channel,Conv2d_Kernel_size_row,Conv2d_Kernel_size_col))\n",
    "    #Weights\n",
    "    for i in range(0,Conv2d_Kernel_number):\n",
    "        txtfile0.write(\"\\t{\\n\")\n",
    "        for j in range(0,Conv2d_Kernel_channel):\n",
    "            txtfile0.write(\"\\t\\t{\\n\")\n",
    "            for k in range(0,Conv2d_Kernel_size_row):\n",
    "                txtfile0.write(\"\\t\\t\\t{\")\n",
    "                for w in range(0,Conv2d_Kernel_size_col):\n",
    "                   txtfile0.write(data_t%(loadData2[i][j][k][w])+\", \") \n",
    "                txtfile0.write(\"},\\n\")  \n",
    "            txtfile0.write(\"\\t\\t},\\n\")\n",
    "        txtfile0.write(\"\\t},\\n\")\n",
    "    #.h File End\n",
    "    txtfile0.write(\"\\n};\")\n",
    "    txtfile0.write(\"\\n#endif\")\n",
    "    txtfile0.close()\n",
    "    #Bias\n",
    "    txtfile1 = open(bias_file_path, 'w',encoding='UTF-8')\n",
    "    #.h File Header\n",
    "    txtfile1.write(\"#ifndef __L%d_DWCONV2D_BIAS_H__\\n\"%(Layer_Number))\n",
    "    txtfile1.write(\"#define __L%d_DWCONV2D_BIAS_H__\\n\"%(Layer_Number))\n",
    "    txtfile1.write(\"#include \\\"opennna_core.h\\\"\\n\")\n",
    "    #数组\n",
    "    txtfile1.write(\"const Bias_t L%d_DwConv2d_Bias[%d]={\\n\"%(Layer_Number,Conv2d_Kernel_channel))\n",
    "    txtfile1.write(\"\\t\")\n",
    "    #Bias\n",
    "    for i in range(0,Conv2d_bias_number):\n",
    "        txtfile1.write(data_t%(Bias[i])+\", \")\n",
    "    #.h File End\n",
    "    txtfile1.write(\"\\n};\")\n",
    "    txtfile1.write(\"\\n#endif\")\n",
    "    txtfile1.close()\n",
    "    print(\"OpenNNA: Depthwise Conv2d Layer:%d Weights[%d]+Bias[%d] Get Success!\"%(Layer_Number,Weights_Index,Bias_Index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606c70a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T04:49:44.947005Z",
     "start_time": "2022-08-24T04:49:44.937007Z"
    }
   },
   "source": [
    "## 10.从Tensorflow(.tflite File)中提取Conv2D的Weights,并生成.h文件\n",
    "与保存到txt文件不同，保存到.h文件将会同时保存当前层的scale和zero point信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6b9245d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T12:17:27.375242Z",
     "start_time": "2022-08-25T12:17:27.364275Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#以下为一个提取Tensorflow Conv2d卷积weights+bias的函数，会以txt保存在程序目录下\n",
    "def OpenNNA_Conv2d_GetWeights_From_TFLite_Save_HFile(Layer_Number,Para,Weights_Index,Bias_Index,precision):\n",
    "    tflite_interpreter = tf.lite.Interpreter(model_path=tflite_model_name)\n",
    "    tflite_interpreter.allocate_tensors()\n",
    "    data_t = precision#\"%8.6f\"\n",
    "    weights_file_path = \"./L\"+str(Layer_Number)+\"_Conv2d_Weights\"+\".h\"\n",
    "    Weights   = tflite_interpreter.tensor(Weights_Index)()\n",
    "    bias_file_path = \"./L\"+str(Layer_Number)+\"_Conv2d_Bias\"+\".h\"\n",
    "    Bias = tflite_interpreter.tensor(Bias_Index)()\n",
    "\n",
    "    Conv2d_Kernel_size_row = Para[0] #卷积核大小\n",
    "    Conv2d_Kernel_size_col = Para[1] #卷积核大小\n",
    "    Conv2d_Kernel_channel = Para[2] #卷积核通道\n",
    "    Conv2d_Kernel_number = Para[3] #卷积核数量\n",
    "    Conv2d_bias_number = Para[4]  #偏置数量\n",
    "\n",
    "    #tensorflow hwc -> chw\n",
    "    loadData0 = np.swapaxes(Weights, 2, 3)\n",
    "    loadData2 = np.swapaxes(loadData0, 1, 2)\n",
    "    #loadData2 = np.swapaxes(loadData1, 0, 1)\n",
    "    txtfile0 = open(weights_file_path, 'w',encoding='UTF-8')\n",
    "    #.h File Header\n",
    "    txtfile0.write(\"#ifndef __L%d_CONV2D_WEIGHTS_H__\\n\"%(Layer_Number))\n",
    "    txtfile0.write(\"#define __L%d_CONV2D_WEIGHTS_H__\\n\"%(Layer_Number))\n",
    "    txtfile0.write(\"#include \\\"opennna_core.h\\\"\\n\")\n",
    "    #数组\n",
    "    txtfile0.write(\"const Weights_t L%d_Conv2d_Weights[%d][%d][%d][%d]={\\n\"%(Layer_Number,Conv2d_Kernel_number,Conv2d_Kernel_channel,Conv2d_Kernel_size_row,Conv2d_Kernel_size_col))\n",
    "    #Weights\n",
    "    for i in range(0,Conv2d_Kernel_number):\n",
    "        txtfile0.write(\"\\t{\\n\")\n",
    "        for j in range(0,Conv2d_Kernel_channel):\n",
    "            txtfile0.write(\"\\t\\t{\\n\")\n",
    "            for k in range(0,Conv2d_Kernel_size_row):\n",
    "                txtfile0.write(\"\\t\\t\\t{\")\n",
    "                for w in range(0,Conv2d_Kernel_size_col):\n",
    "                   txtfile0.write(data_t%(loadData2[i][j][k][w])+\", \") \n",
    "                txtfile0.write(\"},\\n\")  \n",
    "            txtfile0.write(\"\\t\\t},\\n\")\n",
    "        txtfile0.write(\"\\t},\\n\")\n",
    "    #.h File End\n",
    "    txtfile0.write(\"\\n};\")\n",
    "    txtfile0.write(\"\\n#endif\")\n",
    "    txtfile0.close()\n",
    "    #Bias\n",
    "    txtfile1 = open(bias_file_path, 'w',encoding='UTF-8')\n",
    "    #.h File Header\n",
    "    txtfile1.write(\"#ifndef __L%d_CONV2D_BIAS_H__\\n\"%(Layer_Number))\n",
    "    txtfile1.write(\"#define __L%d_CONV2D_BIAS_H__\\n\"%(Layer_Number))\n",
    "    txtfile1.write(\"#include \\\"opennna_core.h\\\"\\n\")\n",
    "    #数组\n",
    "    txtfile1.write(\"const Bias_t L%d_Conv2d_Bias[%d]={\\n\"%(Layer_Number,Conv2d_Kernel_number))\n",
    "    txtfile1.write(\"\\t\")\n",
    "    for i in range(0,Conv2d_bias_number):\n",
    "        txtfile1.write(data_t%(Bias[i])+\", \")\n",
    "    #.h File End\n",
    "    txtfile1.write(\"\\n};\")\n",
    "    txtfile1.write(\"\\n#endif\")\n",
    "    txtfile1.close()\n",
    "    print(\"OpenNNA: Conv2d Layer:%d Weights[%d]+Bias[%d] Get Success!\"%(Layer_Number,Weights_Index,Bias_Index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fba82c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T12:17:29.913635Z",
     "start_time": "2022-08-25T12:17:29.283429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenNNA: Conv2d Layer:0 Weights[8]+Bias[6] Get Success!\n",
      "OpenNNA: Depthwise Conv2d Layer:1 Weights[35]+Bias[34] Get Success!\n",
      "OpenNNA: Conv2d Layer:2 Weights[38]+Bias[36] Get Success!\n",
      "OpenNNA: Depthwise Conv2d Layer:3 Weights[41]+Bias[40] Get Success!\n",
      "OpenNNA: Conv2d Layer:4 Weights[44]+Bias[42] Get Success!\n",
      "OpenNNA: Depthwise Conv2d Layer:5 Weights[47]+Bias[46] Get Success!\n",
      "OpenNNA: Conv2d Layer:6 Weights[50]+Bias[48] Get Success!\n",
      "OpenNNA: Depthwise Conv2d Layer:7 Weights[53]+Bias[52] Get Success!\n",
      "OpenNNA: Conv2d Layer:8 Weights[56]+Bias[54] Get Success!\n",
      "OpenNNA: Depthwise Conv2d Layer:9 Weights[59]+Bias[58] Get Success!\n",
      "OpenNNA: Conv2d Layer:10 Weights[62]+Bias[60] Get Success!\n",
      "OpenNNA: Depthwise Conv2d Layer:11 Weights[65]+Bias[64] Get Success!\n",
      "OpenNNA: Conv2d Layer:12 Weights[68]+Bias[66] Get Success!\n",
      "OpenNNA: Depthwise Conv2d Layer:13 Weights[71]+Bias[70] Get Success!\n",
      "OpenNNA: Conv2d Layer:14 Weights[74]+Bias[72] Get Success!\n",
      "OpenNNA: Depthwise Conv2d Layer:15 Weights[77]+Bias[76] Get Success!\n",
      "OpenNNA: Conv2d Layer:16 Weights[80]+Bias[78] Get Success!\n",
      "OpenNNA: Depthwise Conv2d Layer:17 Weights[83]+Bias[82] Get Success!\n",
      "OpenNNA: Conv2d Layer:18 Weights[86]+Bias[84] Get Success!\n",
      "OpenNNA: Depthwise Conv2d Layer:19 Weights[11]+Bias[10] Get Success!\n",
      "OpenNNA: Conv2d Layer:20 Weights[14]+Bias[12] Get Success!\n",
      "OpenNNA: Depthwise Conv2d Layer:21 Weights[17]+Bias[16] Get Success!\n",
      "OpenNNA: Conv2d Layer:22 Weights[20]+Bias[18] Get Success!\n",
      "OpenNNA: Depthwise Conv2d Layer:23 Weights[23]+Bias[22] Get Success!\n",
      "OpenNNA: Conv2d Layer:24 Weights[26]+Bias[24] Get Success!\n",
      "OpenNNA: Depthwise Conv2d Layer:25 Weights[29]+Bias[28] Get Success!\n",
      "OpenNNA: Conv2d Layer:26 Weights[32]+Bias[30] Get Success!\n",
      "OpenNNA: Conv2d Layer:28 Weights[3]+Bias[2] Get Success!\n"
     ]
    }
   ],
   "source": [
    "#与Tensorflow H5 weights&bias的index需要手动printf检查不同， tflite每层weights&bias的index就是Netron所显示的location\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite_Save_HFile(0,[3,3,3,8,8],8,6,\"%d\")#第0层Conv2d\n",
    "\n",
    "OpenNNA_DWConv2d_GetWeights_From_TFLite_Save_HFile(1,[3,3,8,1,8],35,34,\"%d\")#第1层Depthwise Conv2d\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite_Save_HFile(2,[1,1,8,16,16],38,36,\"%d\")#第2层Conv2d\n",
    "\n",
    "OpenNNA_DWConv2d_GetWeights_From_TFLite_Save_HFile(3,[3,3,16,1,16],41,40,\"%d\")#第3层Depthwise Conv2d\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite_Save_HFile(4,[1,1,16,32,32],44,42,\"%d\")#第4层Conv2d\n",
    "\n",
    "OpenNNA_DWConv2d_GetWeights_From_TFLite_Save_HFile(5,[3,3,32,1,32],47,46,\"%d\")#第5层Depthwise Conv2d\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite_Save_HFile(6,[1,1,32,32,32],50,48,\"%d\")#第6层Conv2d\n",
    "\n",
    "OpenNNA_DWConv2d_GetWeights_From_TFLite_Save_HFile(7,[3,3,32,1,32],53,52,\"%d\")#第7层Depthwise Conv2d\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite_Save_HFile(8,[1,1,32,64,64],56,54,\"%d\")#第8层Conv2d\n",
    "\n",
    "OpenNNA_DWConv2d_GetWeights_From_TFLite_Save_HFile(9,[3,3,64,1,64],59,58,\"%d\")#第9层Depthwise Conv2d\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite_Save_HFile(10,[1,1,64,64,64],62,60,\"%d\")#第10层Conv2d\n",
    "\n",
    "OpenNNA_DWConv2d_GetWeights_From_TFLite_Save_HFile(11,[3,3,64,1,64],65,64,\"%d\")#第11层Depthwise Conv2d\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite_Save_HFile(12,[1,1,64,128,128],68,66,\"%d\")#第12层Conv2d\n",
    "\n",
    "OpenNNA_DWConv2d_GetWeights_From_TFLite_Save_HFile(13,[3,3,128,1,128],71,70,\"%d\")#第13层Depthwise Conv2d\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite_Save_HFile(14,[1,1,128,128,128],74,72,\"%d\")#第14层Conv2d\n",
    "\n",
    "OpenNNA_DWConv2d_GetWeights_From_TFLite_Save_HFile(15,[3,3,128,1,128],77,76,\"%d\")#第15层Depthwise Conv2d\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite_Save_HFile(16,[1,1,128,128,128],80,78,\"%d\")#第16层Conv2d\n",
    "\n",
    "OpenNNA_DWConv2d_GetWeights_From_TFLite_Save_HFile(17,[3,3,128,1,128],83,82,\"%d\")#第17层Depthwise Conv2d\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite_Save_HFile(18,[1,1,128,128,128],86,84,\"%d\")#第18层Conv2d\n",
    "\n",
    "OpenNNA_DWConv2d_GetWeights_From_TFLite_Save_HFile(19,[3,3,128,1,128],11,10,\"%d\")#第19层Depthwise Conv2d\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite_Save_HFile(20,[1,1,128,128,128],14,12,\"%d\")#第20层Conv2d\n",
    "\n",
    "OpenNNA_DWConv2d_GetWeights_From_TFLite_Save_HFile(21,[3,3,128,1,128],17,16,\"%d\")#第21层Depthwise Conv2d\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite_Save_HFile(22,[1,1,128,128,128],20,18,\"%d\")#第22层Conv2d\n",
    "\n",
    "OpenNNA_DWConv2d_GetWeights_From_TFLite_Save_HFile(23,[3,3,128,1,128],23,22,\"%d\")#第23层Depthwise Conv2d\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite_Save_HFile(24,[1,1,128,256,256],26,24,\"%d\")#第24层Conv2d\n",
    "\n",
    "OpenNNA_DWConv2d_GetWeights_From_TFLite_Save_HFile(25,[3,3,256,1,256],29,28,\"%d\")#第25层Depthwise Conv2d\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite_Save_HFile(26,[1,1,256,256,256],32,30,\"%d\")#第26层Conv2d\n",
    "\n",
    "#第27层AvgPool\n",
    "\n",
    "OpenNNA_Conv2d_GetWeights_From_TFLite_Save_HFile(28,[1,1,256,1001,1001],3,2,\"%d\")#第28层Conv2d\n",
    "\n",
    "#Output:1001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0069f449",
   "metadata": {},
   "source": [
    "### 将ImageNet的.jpg转换为c 数组存到.txt中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e223df27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T13:34:31.747043Z",
     "start_time": "2022-08-25T13:34:28.473180Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "import numpy as np\n",
    "\n",
    "channel = 3\n",
    "row = 128\n",
    "col = 128\n",
    "\n",
    "image_value = tf.io.read_file('./imagenet/tiger.jpg')\n",
    "\n",
    "#解码为tensor\n",
    "image_value = tf.io.decode_jpeg(image_value,channels = 3)\n",
    "\n",
    "\n",
    "image_value = tf.image.resize(image_value, (row,col))#改变像素值为128*128\n",
    "\n",
    "\n",
    "#tensor转array\n",
    "image_value = image_value.numpy()\n",
    "\n",
    "\n",
    "loadData1 = np.swapaxes(image_value, 0, 2)\n",
    "loadData2 = np.swapaxes(loadData1, 1, 2)\n",
    "\n",
    "\n",
    "txtfile = open(r'ILSVRC2012_val_00000005_uint8.txt', 'w',encoding='UTF-8')\n",
    "for i in range(0,channel):\n",
    "    txtfile.write(\"\\t{\\n\")\n",
    "    for j in range(0,row):\n",
    "        txtfile.write(\"\\t\\t{\")\n",
    "        for k in range(0,col):\n",
    "            #txtfile.write(str(loadData2[i][j][k])+\", \") \n",
    "            txtfile.write(\"%d\"%(loadData2[i][j][k])+\", \") \n",
    "        txtfile.write(\"},\\n\")\n",
    "    txtfile.write(\"\\t},\\n\\n\")\n",
    "txtfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac418fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
